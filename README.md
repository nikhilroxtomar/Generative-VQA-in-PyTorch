# üß† Generative Visual Question Answering (VQA) using CLIP + GPT-2

This repository contains an **end-to-end Generative Visual Question Answering (VQA) pipeline** built using **CLIP (ViT-B/32)** for image understanding and **GPT-2** for question encoding and answer generation.

The project follows a **two-stage training strategy**:
1. **Feature Extraction (Frozen Encoders)**
2. **Selective Fine-Tuning (Unfreezing Last Layers)**

This implementation is designed for **research, learning, and reproducibility**, and is intentionally written in **clean PyTorch** without excessive abstractions.

---

## üöÄ Key Features

- üîç **CLIP-based image encoding**
- üìù **GPT-2 based question encoding**
- üîó **Gated multimodal fusion**
- üß† **GRU-based generative answer decoder**
- üéØ **Teacher forcing during training**
- ‚ö° **Mixed precision training (AMP)**
- üìä **BLEU score evaluation**
- üñºÔ∏è **Prediction visualization**
- üß™ **Mini VQA v2 dataset creation script**

---

üé• **YouTube Playlist (Walkthrough & Demo):**  
üëâ https://www.youtube.com/playlist?list=PLHYn9gDxQOphmmPsayzbdKnWdEupt8gHG

Watch the videos to understand architecture, training, and code walkthrough step-by-step!

---

## üìÅ Repository Structure
```
.
‚îú‚îÄ‚îÄ mini_vqa_v2.py # Dataset preparation from VQA v2
‚îú‚îÄ‚îÄ model.py # CLIP + GPT-2 VQA model
‚îú‚îÄ‚îÄ vocab.py # Answer vocabulary handling
‚îú‚îÄ‚îÄ preprocess.py # Dataset & vocab utilities
‚îú‚îÄ‚îÄ train.py # Feature extraction training
‚îú‚îÄ‚îÄ fine_tune.py # Selective fine-tuning
‚îú‚îÄ‚îÄ test.py # Testing, BLEU score & visualization
‚îú‚îÄ‚îÄ output/
‚îÇ ‚îú‚îÄ‚îÄ feature_extraction/
‚îÇ ‚îú‚îÄ‚îÄ fine_tuning/
‚îÇ ‚îî‚îÄ‚îÄ vqa_test_results.csv
‚îî‚îÄ‚îÄ results/ # Prediction visualizations
```

---

## üß† Model Architecture Overview

### üîπ Image Encoder
- **CLIP ViT-B/32**
- Frozen during feature extraction
- Last *N* layers unfrozen during fine-tuning

### üîπ Question Encoder
- **GPT-2 (distilgpt2)**
- Mean-pooled token embeddings
- Last *N* transformer blocks unfrozen during fine-tuning

### üîπ Fusion
- Gated fusion mechanism
- Followed by refinement MLP

### üîπ Answer Decoder
- GRU-based auto-regressive decoder
- Word-level answer generation

---

## üß™ Dataset

This project uses a **mini version of VQA v2**, created using:

- MS COCO 2014 images
- VQA v2 Open-Ended questions
- Most frequent human answer as ground truth
- Filtered short/ambiguous answers

### Create Dataset
```bash
python mini_vqa_v2.py
```
This generates:
```
mini_vqa_v2/
‚îú‚îÄ‚îÄ images/
‚îú‚îÄ‚îÄ metadata.csv
‚îî‚îÄ‚îÄ qa_pairs.json
```

## Outputs:
- üìÑ vqa_test_results.csv
- üìä Mean BLEU score (via sacrebleu)
- üñºÔ∏è Saved qualitative visualisations

## Metrics
- BLEU Score (Generative evaluation)
- Qualitative inspection via image + text overlays

> ‚ö†Ô∏è Note: VQA accuracy is not ideal for generative settings, hence BLEU is used.

---

License

This project is released under the MIT License.
Feel free to use, modify, and cite for academic or educational purposes.

‚≠ê If you found this project useful, consider starring the repository!



